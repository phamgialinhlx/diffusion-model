# @package _global_

defaults:
  - override /data: celeba.yaml
  - override /model: diffusion.yaml
  - override /callbacks: diffusion_callbacks.yaml
  - override /trainer: gpu.yaml

tags: ["celeba", "ddim"]

data:
  num_workers: 6
  batch_size: 32

model:
  # autoencoder_ckpt_path: "/work/hpc/pgl/lung-diffusion/outputs/vqmodel_f16_celeba/checkpoints/last.ckpt"
  autoencoder_ckpt_path: "/work/hpc/pgl/lung-diffusion/outputs/vqmodel_f16_celeba/checkpoints/epoch_171.ckpt"
  sampler:
    _target_: src.models.diffusion.sampler.ddim.DDIMSampler
    clip_denoised: False
  net:
    _target_: src.models.diffusion.unet.UNetModel
    image_size: 16
    in_channels: 16
    out_channels: 16
    model_channels: 192
    channel_mult: [1, 2, 4]
    attention_resolutions: [1, 2, 4, 8]
    num_heads: 8
    num_res_blocks: 2
  # optimizer:
  #   _target_: torch.optim.AdamW
  #   _partial_: true
  #   lr: 0.0005
  
  # scheduler:
  #   _target_: src.utils.lr_scheduler.LambdaWarmUpScheduler3
  #   warm_up_steps: 5000

callbacks:
  log_image:
    frequency: 1
