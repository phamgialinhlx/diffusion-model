# @package _global_

# to execute this experiment run:
# python train.py experiment=example

defaults:
  - override /data: mnist.yaml
  - override /model: vqvae.yaml
  - override /callbacks: ae_callbacks.yaml
  - override /trainer: gpu.yaml

# all parameters below will be merged with parameters from default configurations set above
# this allows you to overwrite only specified parameters

model:
  loss: 
    _target_: src.models.modules.losses.vqperceptual.VQLPIPSWithDiscriminator
    disc_start: 250001
    codebook_weight: 1.0
    disc_num_layers: 2
    disc_in_channels: ${data.image_channels}
    disc_weight: 0.75
    disc_conditional: false
    n_classes: ${data.n_classes}
  autoencoderconfig:
    channels: ${data.image_size}
    channel_multipliers: [1, 2]
    n_resnet_blocks: 2
    img_channels: ${data.image_channels}
    z_channels: 16
    double_z: False
    resolution: ${data.image_size}
    attn_resolutions:
    - 16
    out_img_size: 16
    out_channels: 16
  
data:
  num_workers: 24
  batch_size: 64
  train_val_test_split: [128, 128, 128]
  
callbacks:
  log_image:
    frequency: 1

task_name: "develop"
